{
  "permissions": {
    "allow": [
      "Bash(mkdir:*)",
      "Bash(powershell:*)",
      "Bash(dir:*)",
      "Bash(cmd /c:*)",
      "Bash(uv sync:*)",
      "Bash(uv run:*)",
      "Bash(curl:*)",
      "Bash(uv add:*)",
      "WebSearch",
      "WebFetch(domain:docs.astral.sh)",
      "Bash(mv:*)",
      "Bash(Start-Sleep -Seconds 3)",
      "Bash(timeout:*)",
      "Bash(rm:*)",
      "Bash(.\\dist\\fluid-server.exe --help)",
      "Bash(uv pip install:*)",
      "Bash(python:*)",
      "Bash(distfluid-server.exe --help)",
      "Bash(xcopy:*)",
      "Bash(robocopy:*)",
      "Bash(set PYTHONPATH=src)",
      "WebFetch(domain:huggingface.co)",
      "Bash(.\\scripts\\build.ps1)",
      "Bash(.fluid-server.exe --help)",
      "Bash(.\\dist\\fluid-server.exe:*)",
      "Bash(copy:*)",
      "Bash(PYTHONPATH=src uv run python -m fluid_server --host 127.0.0.1 --port 8083 --llm-model qwen3-8b-int8-ov --whisper-model whisper-large-v3-turbo-fp16-ov-npu)",
      "Bash(PYTHONPATH=src uv run python -m fluid_server --host 127.0.0.1 --port 8083 --llm-model qwen3-8b-int8-ov --whisper-model whisper-large-v3-turbo-fp16-ov-npu --model-path \"C:\\Users\\brand\\AppData\\Local\\Slipbox\\Models\")",
      "Bash(Start-Sleep -Seconds 10)",
      "Bash(.\\dist\\fluid-server.exe:*)",
      "Bash(.\\dist\\fluid-server.exe:*)",
      "Bash(\"C:\\Users\\brand\\code\\fluid-server\\dist\\fluid-server.exe\" --host 127.0.0.1 --port 8084 --llm-model qwen3-8b-int8-ov --whisper-model whisper-large-v3-turbo-fp16-ov-npu --model-path \"C:\\Users\\brand\\AppData\\Local\\Slipbox\\Models\")",
      "Bash(PYTHONPATH=src uv run python -c \"\nimport openvino as ov\ncore = ov.Core()\ndevices = core.available_devices\nprint(''Available devices:'', devices)\nfor device in devices:\n    if ''GPU'' in device:\n        print(f''GPU device: {device}'')\n        props = core.get_property(device, ov.properties.supported_properties)\n        print(f''  Properties: {len(props)} available'')\n\")",
      "Bash(PYTHONPATH=src uv run python -c \"\nimport openvino as ov\ncore = ov.Core()\ntry:\n    # Check if GPU supports cache_dir property\n    gpu_props = core.get_property(''GPU'', ov.properties.supported_properties)\n    print(''GPU supported properties:'')\n    for prop in gpu_props:\n        if ''cache'' in str(prop).lower():\n            print(f''  {prop}'')\n    \n    # Try to check cache_dir support specifically\n    try:\n        cache_support = ov.properties.cache_dir in gpu_props\n        print(f''\\nGPU cache_dir support: {cache_support}'')\n    except Exception as e:\n        print(f''\\nError checking cache_dir: {e}'')\n    \n    # Compare with CPU\n    print(''\\nCPU supported properties with cache:'')\n    cpu_props = core.get_property(''CPU'', ov.properties.supported_properties)\n    for prop in cpu_props:\n        if ''cache'' in str(prop).lower():\n            print(f''  {prop}'')\n            \nexcept Exception as e:\n    print(f''Error: {e}'')\n\")",
      "Bash(PYTHONPATH=src uv run python -m fluid_server --host 127.0.0.1 --port 8084 --llm-model qwen3-8b-int8-ov --whisper-model whisper-large-v3-turbo-fp16-ov-npu --model-path \"C:\\Users\\brand\\AppData\\Local\\Slipbox\\Models\")",
      "Bash(PYTHONPATH=src uv run python -m fluid_server --host 127.0.0.1 --port 8085 --llm-model qwen3-8b-int8-ov --whisper-model whisper-large-v3-turbo-fp16-ov-npu --model-path \"C:\\Users\\brand\\AppData\\Local\\Slipbox\\Models\")",
      "Bash(taskkill:*)",
      "Bash(cp:*)",
      "Bash(PYTHONPATH=src uv run python -m fluid_server --help)",
      "Bash(.venvScriptspython.exe -m PyInstaller --version)",
      "Bash(\".venv\\Scripts\\python.exe\" -m PyInstaller --version)",
      "Bash(\".venv\\Scripts\\python.exe\" -m PyInstaller --onefile --name fluid-server --noconsole --hidden-import=openvino --hidden-import=openvino_genai --hidden-import=openvino_tokenizers --hidden-import=openvino.runtime --hidden-import=openvino.properties --hidden-import=uvicorn.logging --hidden-import=uvicorn.loops --hidden-import=uvicorn.loops.auto --hidden-import=uvicorn.protocols --hidden-import=uvicorn.protocols.http --hidden-import=uvicorn.protocols.http.auto --hidden-import=uvicorn.protocols.websockets --hidden-import=uvicorn.protocols.websockets.auto --hidden-import=uvicorn.lifespan --hidden-import=uvicorn.lifespan.on --hidden-import=uvicorn.lifespan.off --collect-all=openvino --collect-all=openvino_genai --collect-all=openvino_tokenizers --log-level=INFO \"src\\fluid_server\\__main__.py\")",
      "Bash(tasklist)",
      "Bash(\".\\fluid-server.exe\" --help)",
      "Bash(\".venv/Scripts/python.exe\" -m PyInstaller --onefile --name fluid-server --noconsole --hidden-import=openvino --hidden-import=openvino_genai --hidden-import=openvino_tokenizers --hidden-import=openvino.runtime --hidden-import=openvino.properties --hidden-import=uvicorn.logging --hidden-import=uvicorn.loops --hidden-import=uvicorn.loops.auto --hidden-import=uvicorn.protocols --hidden-import=uvicorn.protocols.http --hidden-import=uvicorn.protocols.http.auto --hidden-import=uvicorn.protocols.websockets --hidden-import=uvicorn.protocols.websockets.auto --hidden-import=uvicorn.lifespan --hidden-import=uvicorn.lifespan.on --hidden-import=uvicorn.lifespan.off --collect-all=openvino --collect-all=openvino_genai --collect-all=openvino_tokenizers --log-level=INFO \"src\\fluid_server\\__main__.py\")",
      "Bash(../windows/fluid-server.exe --help)",
      "Bash(Start-Sleep -Seconds 5)",
      "Bash(\".venv/Scripts/python.exe\" -m PyInstaller --onefile --name fluid-server --noconsole --hidden-import=openvino --hidden-import=openvino_genai --hidden-import=openvino_tokenizers --hidden-import=openvino.runtime --hidden-import=openvino.properties --hidden-import=uvicorn.logging --hidden-import=uvicorn.loops --hidden-import=uvicorn.loops.auto --hidden-import=uvicorn.protocols --hidden-import=uvicorn.protocols.http --hidden-import=uvicorn.protocols.http.auto --hidden-import=uvicorn.protocols.websockets --hidden-import=uvicorn.protocols.websockets.auto --hidden-import=uvicorn.lifespan --hidden-import=uvicorn.lifespan.on --hidden-import=uvicorn.lifespan.off --hidden-import=librosa --hidden-import=scipy --hidden-import=scipy.signal --hidden-import=scipy.stats --hidden-import=scipy.stats._distn_infrastructure --hidden-import=scipy.stats._stats --hidden-import=scipy.stats.distributions --hidden-import=scipy.io --hidden-import=scipy.fft --hidden-import=numpy --collect-all=openvino --collect-all=openvino_genai --collect-all=openvino_tokenizers --collect-all=librosa --collect-all=scipy --log-level=INFO \"src\\fluid_server\\__main__.py\")",
      "Bash(\".venv/Scripts/python.exe\" -m PyInstaller --onefile --name fluid-server --noconsole --hidden-import=openvino --hidden-import=openvino_genai --hidden-import=openvino_tokenizers --hidden-import=openvino.runtime --hidden-import=openvino.properties --hidden-import=uvicorn.logging --hidden-import=uvicorn.loops --hidden-import=uvicorn.loops.auto --hidden-import=uvicorn.protocols --hidden-import=uvicorn.protocols.http --hidden-import=uvicorn.protocols.http.auto --hidden-import=uvicorn.protocols.websockets --hidden-import=uvicorn.protocols.websockets.auto --hidden-import=uvicorn.lifespan --hidden-import=uvicorn.lifespan.on --hidden-import=uvicorn.lifespan.off --hidden-import=numpy --collect-all=openvino --collect-all=openvino_genai --collect-all=openvino_tokenizers --log-level=INFO \"src\\fluid_server\\__main__.py\")",
      "Bash(PYTHONPATH=src timeout 10 uv run python -m fluid_server --workers 2 --port 8087 --no-warm-up --model-path \"C:\\Users\\brand\\AppData\\Local\\Slipbox\\Models\")",
      "Bash(\".venv/Scripts/python.exe\" -m PyInstaller --version)",
      "Bash(\".venv/Scripts/python.exe\" -m PyInstaller --onefile --name fluid-server --noconsole --hidden-import=openvino --hidden-import=openvino_genai --hidden-import=openvino_tokenizers --hidden-import=openvino.runtime --hidden-import=openvino.properties --hidden-import=uvicorn.logging --hidden-import=uvicorn.loops --hidden-import=uvicorn.loops.auto --hidden-import=uvicorn.protocols --hidden-import=uvicorn.protocols.http --hidden-import=uvicorn.protocols.http.auto --hidden-import=uvicorn.protocols.websockets --hidden-import=uvicorn.protocols.websockets.auto --hidden-import=uvicorn.lifespan --hidden-import=uvicorn.lifespan.on --hidden-import=uvicorn.lifespan.off --hidden-import=fluid_server.app --hidden-import=librosa --hidden-import=scipy --hidden-import=scipy.signal --hidden-import=scipy.stats --hidden-import=scipy.stats._distn_infrastructure --hidden-import=scipy.stats._stats --hidden-import=scipy.stats.distributions --hidden-import=scipy.io --hidden-import=scipy.fft --hidden-import=numpy --collect-all=openvino --collect-all=openvino_genai --collect-all=openvino_tokenizers --collect-all=librosa --collect-all=scipy --log-level=INFO \"src\\fluid_server\\__main__.py\")",
      "Bash(\".venv/Scripts/python.exe\" -m PyInstaller --onefile --name fluid-server --noconsole --hidden-import=fluid_server.app --hidden-import=uvicorn.logging --hidden-import=uvicorn.loops --hidden-import=uvicorn.loops.auto --hidden-import=uvicorn.protocols --hidden-import=uvicorn.protocols.http --hidden-import=uvicorn.protocols.http.auto --hidden-import=uvicorn.protocols.websockets --hidden-import=uvicorn.protocols.websockets.auto --hidden-import=uvicorn.lifespan --hidden-import=uvicorn.lifespan.on --hidden-import=uvicorn.lifespan.off --log-level=WARN \"src\\fluid_server\\__main__.py\")",
      "Bash(\".venv/Scripts/python.exe\" -m PyInstaller --onefile --name fluid-server --noconsole --hidden-import=fluid_server.app --hidden-import=uvicorn.logging --hidden-import=uvicorn.loops --hidden-import=uvicorn.loops.auto --hidden-import=uvicorn.protocols --hidden-import=uvicorn.protocols.http --hidden-import=uvicorn.protocols.http.auto --hidden-import=uvicorn.protocols.websockets --hidden-import=uvicorn.protocols.websockets.auto --hidden-import=uvicorn.lifespan --hidden-import=uvicorn.lifespan.on --hidden-import=uvicorn.lifespan.off --log-level=ERROR \"src\\fluid_server\\__main__.py\")",
      "Bash(PYTHONPATH=src timeout 15 uv run python -m fluid_server --host 127.0.0.1 --port 8092 --workers 2 --no-warm-up --model-path \"C:\\Users\\brand\\AppData\\Local\\Slipbox\\Models\")",
      "Bash(PYTHONPATH=src timeout 15 uv run python -m fluid_server --host 127.0.0.1 --port 8093 --workers 1 --no-warm-up --llm-model qwen3-8b-int8-ov --model-path \"C:\\Users\\brand\\AppData\\Local\\Slipbox\\Models\")",
      "Bash(chmod:*)",
      "Bash(.\\scripts\\typecheck.ps1)",
      "Bash(PYTHONPATH=src timeout 15 uv run python -m fluid_server --help)",
      "Bash(PYTHONPATH=src uv run python -c \"\nfrom fluid_server.runtimes.qnn_whisper import QNNWhisperRuntime\nfrom fluid_server.utils.model_discovery import ModelDiscovery\nprint(''✓ QNN runtime import successful'')\nprint(''✓ ModelDiscovery with QNN support successful'')\nprint(''QNN integration is ready!'')\n\")",
      "Bash(PYTHONPATH=src uv run python -c \"\nfrom fluid_server.runtimes.qnn_whisper import QNNWhisperRuntime\nfrom fluid_server.utils.model_discovery import ModelDiscovery\nprint(''QNN runtime import successful'')\nprint(''ModelDiscovery with QNN support successful'')\nprint(''QNN integration is ready!'')\n\")",
      "Bash(PYTHONPATH=src uv run python -c \"\nfrom pathlib import Path\nfrom fluid_server.utils.model_discovery import ModelDiscovery\n\n# Test QNN model detection\ntest_path = Path(''test_models/whisper/whisper-large-v3-turbo-qnn'')\ntest_path.mkdir(parents=True, exist_ok=True)\n\n# Create fake QNN model structure\n(test_path / ''snapdragon-x-elite/encoder'').mkdir(parents=True, exist_ok=True)\n(test_path / ''snapdragon-x-elite/decoder'').mkdir(parents=True, exist_ok=True)\n(test_path / ''snapdragon-x-elite/encoder/model.onnx'').touch()\n(test_path / ''snapdragon-x-elite/decoder/model.onnx'').touch()\n\n# Test detection\nis_valid = ModelDiscovery._validate_qnn_whisper_model(test_path)\nruntime_type = ModelDiscovery.get_whisper_runtime_type(test_path)\n\nprint(f''QNN model detection: {is_valid}'')\nprint(f''Runtime type: {runtime_type}'')\n\n# Cleanup\nimport shutil\nshutil.rmtree(''test_models'', ignore_errors=True)\n\")",
      "Bash(\"./dist/fluid-server.exe\" --help)",
      "Bash(\".venv/Scripts/python.exe\" -m PyInstaller --onefile --name fluid-server --noconsole \"src\\fluid_server\\__main__.py\")",
      "Bash(\"scripts\\kill_server.bat\")",
      "mcp__deepwiki__read_wiki_structure",
      "mcp__deepwiki__read_wiki_contents",
      "mcp__deepwiki__ask_question"
    ],
    "deny": [],
    "ask": [],
    "additionalDirectories": [
      "C:\\Users\\brand\\AppData\\Local\\Slipbox\\Models\\cache\\llm\\qwen3-8b-int8-ov",
      "C:\\Users\\brand"
    ]
  }
}